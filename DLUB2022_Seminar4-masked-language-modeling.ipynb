{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Deep Learning Ulaanbaatar (DLUB) 2022 - Summer School üá≤üá≥\n\n**Seminar: Mongolian Masked Language Modeling using HuggingFace Transformers**\n\n–ë–∏–¥ –Ω–∞—Ä —é—É —Ö–∏–π—Ö –≥—ç–∂ –±–∞–π–≥–∞–∞ –≤—ç?\n```python\ndataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_mn\", split=\"train\")\ntokenizer = BertTokenizerFast.from_pretrained('./dlub')\nmodel = AutoModelForMaskedLM.from_config(config)\n\n```\n\n\nToday we cover:\n- [ ] HuggingFace `transformers`, `tokenizers` and `datasets` libraries\n- [ ] –ê—à–∏–≥–ª–∞—Ö ”©–≥”©–≥–¥”©–ª - –¥–∞—Ç–∞\n- [ ] Transformers - `Config`\n- [ ] Tokenization and `BertTokenizer`\n- [ ] –î–∞—Ç–∞ –±—ç–ª—Ç–≥—ç–ª\n- [ ] –°—É—Ä–≥–∞–ª—Ç\n- [ ] Push it to HuggingFace model hub\n","metadata":{}},{"cell_type":"code","source":"# for huggingface hub integration\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\n\n!apt install git-lfs\n!git lfs install","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:17:43.868794Z","iopub.execute_input":"2022-06-30T02:17:43.869266Z","iopub.status.idle":"2022-06-30T02:17:52.118796Z","shell.execute_reply.started":"2022-06-30T02:17:43.869144Z","shell.execute_reply":"2022-06-30T02:17:52.117364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers, datasets, tokenizers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T02:17:52.122322Z","iopub.execute_input":"2022-06-30T02:17:52.12285Z","iopub.status.idle":"2022-06-30T02:18:00.444567Z","shell.execute_reply.started":"2022-06-30T02:17:52.122803Z","shell.execute_reply":"2022-06-30T02:18:00.443333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=false","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:00.446289Z","iopub.execute_input":"2022-06-30T02:18:00.448401Z","iopub.status.idle":"2022-06-30T02:18:00.457317Z","shell.execute_reply.started":"2022-06-30T02:18:00.448326Z","shell.execute_reply":"2022-06-30T02:18:00.455719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.__version__, datasets.__version__, tokenizers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:00.46174Z","iopub.execute_input":"2022-06-30T02:18:00.462543Z","iopub.status.idle":"2022-06-30T02:18:00.4768Z","shell.execute_reply.started":"2022-06-30T02:18:00.462501Z","shell.execute_reply":"2022-06-30T02:18:00.475266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –ê—à–∏–≥–ª–∞—Ö ”©–≥”©–≥–¥”©–ª - –¥–∞—Ç–∞\n\n–ë–∏–¥–Ω–∏–π –∞—à–∏–≥–ª–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç –±–æ–ª Common Crawl –∞–∞—Å —Ü—ç–≤—ç—Ä–ª—ç–∂ –∞–≤—Å–∞–Ω OSCAR (Open Super-large Crawled ALMAnaCH coRpus).\n\nhugginface-—ã–Ω `datasets` library-–≥ –∞—à–∏–≥–ª–∞–Ω –¥–∞—Ç–∞–≥–∞–∞ —Ç–∞—Ç–∞—Ö –±–æ–ª–æ–Ω —Ç“Ø“Ø–Ω –¥—ç—ç—Ä –ø—Ä–æ—Ü–µ—Å—Å —Ö–∏–π—Ö –∏–ª“Ø“Ø –∞–º–∞—Ä—Ö–∞–Ω –±–æ–ª—Å–æ–Ω –±–∞–π–Ω–∞.\n\n\n- OSCAR dataset view: https://huggingface.co/datasets/oscar/viewer/unshuffled_deduplicated_mn/train\n- datasets library: https://github.com/huggingface/datasets\n- examples for splits: https://huggingface.co/docs/datasets/v1.11.0/splits.html#examples","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n# dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_mn\", split=\"train\") # -> –±“Ø—Ç–Ω—ç—ç—Ä –Ω—å –∞–≤–∞—Ö\n# dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_mn\", split=\"train[:5%]\") # -> —ç—Ö–Ω–∏–π 5% –∏–π–≥ –∞–≤–∞—Ö\ndataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_mn\", split=\"train[:200]\") # -> —ç—Ö–Ω–∏–π 200 ”©–≥”©–≥–¥–ª–∏–π–≥ –∞–≤–∞—Ö","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:00.479085Z","iopub.execute_input":"2022-06-30T02:18:00.480467Z","iopub.status.idle":"2022-06-30T02:18:42.515232Z","shell.execute_reply.started":"2022-06-30T02:18:00.480334Z","shell.execute_reply":"2022-06-30T02:18:42.513716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:42.521788Z","iopub.execute_input":"2022-06-30T02:18:42.524746Z","iopub.status.idle":"2022-06-30T02:18:42.537182Z","shell.execute_reply.started":"2022-06-30T02:18:42.524699Z","shell.execute_reply":"2022-06-30T02:18:42.535879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformers - `Config`","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'dlub-2022-mlm-full'\nmodel_dir = 'dlub'\n%mkdir $model_dir","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:42.551884Z","iopub.execute_input":"2022-06-30T02:18:42.552742Z","iopub.status.idle":"2022-06-30T02:18:43.552061Z","shell.execute_reply.started":"2022-06-30T02:18:42.552698Z","shell.execute_reply":"2022-06-30T02:18:43.550368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertConfig\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\nconfig.save_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:43.555769Z","iopub.execute_input":"2022-06-30T02:18:43.557376Z","iopub.status.idle":"2022-06-30T02:18:44.663214Z","shell.execute_reply.started":"2022-06-30T02:18:43.557341Z","shell.execute_reply":"2022-06-30T02:18:44.661888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat $model_dir/config.json","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:44.665072Z","iopub.execute_input":"2022-06-30T02:18:44.666543Z","iopub.status.idle":"2022-06-30T02:18:45.446031Z","shell.execute_reply.started":"2022-06-30T02:18:44.666483Z","shell.execute_reply":"2022-06-30T02:18:45.444535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token-–æ–æ model hub —Ä“Ø“Ø –æ—Ä—É—É–ª–∞—Ö\nconfig.push_to_hub(\n    MODEL_NAME,\n    use_auth_token=hf_token\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:45.453066Z","iopub.execute_input":"2022-06-30T02:18:45.453821Z","iopub.status.idle":"2022-06-30T02:18:58.610805Z","shell.execute_reply.started":"2022-06-30T02:18:45.453782Z","shell.execute_reply":"2022-06-30T02:18:58.609418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and `BertTokenizer`\n\nWordPiece tokenizer- BERT-–∏–π–Ω –∞–Ω—Ö–Ω—ã —Ö—É–≤–∏–ª–±–∞—Ä –¥—ç—ç—Ä –∞—à–∏–≥–ª–∞–≥–¥—Å–∞–Ω –±–∞ —Ö—ç–ª–Ω—ç—ç—Å –∏–ª —Ö–∞–º–∞–∞—Ä—Å–∞–Ω tokenizer “Ø“Ø—Å–≥—ç—Ö –∞—Ä–≥–∞ —é–º. \n\n–ê–Ω—Ö —Ç–∞–Ω–∏–ª—Ü—É—É–ª–∞–≥–¥—Å–∞–Ω paper: [Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)\n\n–º–∞—à –æ–ª–æ–Ω tokenizer “Ø“Ø–¥ –∞—à–∏–≥–ª–∞–≥–¥–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π –±–∞ “Ø“Ø–Ω–∏–π–≥ –Ω—ç–≥—Ç–≥—ç–Ω –∞—à–∏–≥–ª–∞—Ö–∞–¥ –∞–º–∞—Ä –±–æ–ª–≥–æ—Å–æ–Ω library –Ω—å [tokenizers](https://github.com/huggingface/tokenizers) —é–º.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom tokenizers import BertWordPieceTokenizer\n\ntokenizer = BertWordPieceTokenizer()\n\ndef batch_iterator(batch_size=1000):\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i: i + batch_size][\"text\"]\n\n# Customized training\ntokenizer.train_from_iterator(batch_iterator(), vocab_size=30522, min_frequency=2, special_tokens=[\n    \"[UNK]\",\n    \"[SEP]\",\n    \"[PAD]\",\n    \"[CLS]\",\n    \"[MASK]\",\n])\n\n# Save files to disk\ntokenizer.save(f\"{model_dir}/tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:18:58.613015Z","iopub.execute_input":"2022-06-30T02:18:58.613966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nbert_tokenizer = BertTokenizerFast.from_pretrained('./dlub')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer.encode_plus('[CLS] –±–∏ –º–æ–Ω–≥–æ–ª —É–ª—Å—ã–Ω –∏—Ä–≥—ç–Ω')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer.save_pretrained(MODEL_NAME, push_to_hub=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –î–∞—Ç–∞ –±—ç–ª—Ç–≥—ç–ª","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return bert_tokenizer(examples[\"text\"])\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"id\", \"text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_datasets[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# block_size = tokenizer.model_max_length\nblock_size = 128\n\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_datasets = tokenized_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=1000,\n    num_proc=2,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –°—É—Ä–≥–∞–ª—Ç","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\nmodel = AutoModelForMaskedLM.from_config(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    MODEL_NAME,\n    num_train_epochs=10,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32*2,\n    dataloader_num_workers=2,\n\n    evaluation_strategy = \"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    save_total_limit=10,\n    report_to='tensorboard',\n\n    # automatic version handling with huggingface\n    push_to_hub=True,\n    hub_token=hf_token,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm_probability=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets,\n    eval_dataset=lm_datasets,\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}